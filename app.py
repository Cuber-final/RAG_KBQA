import os
from typing import List
import gradio as gr
import nltk
from agent.main_agents.chatglm_know import KnowledgeBasedChatLLMApi
from agent.tools.web_serarch import *
from configs.model_config import *

nltk.data.path = [os.path.join(os.path.dirname(__file__), "nltk_data")] + nltk.data.path

llm_model_list = []
llm_model_dict = LLM_MODEL_LIST
init_embedding_model = EMBEDDING_MODEL
init_llm = LLM_MODEL
for model, model_description in llm_model_dict.items():
    # print(model)
    llm_model_list.append(model_description["name"])


def update_status(history, status):
    history = history + [[None, status]]
    print(status)
    return history


chat_llm = KnowledgeBasedChatLLMApi()


def init_model():
    try:
        print("å¼€å§‹åŠ è½½æ¨¡å‹é…ç½®")
        chat_llm.init_model_config()
        print("æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        chat_llm.llm.invoke("ä½ å¥½")
        return """åˆå§‹æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å‡ºé”™: {e}")  # æ‰“å°è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
        return """æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œè¯·é‡æ–°é€‰æ‹©æ¨¡å‹åç‚¹å‡»"é‡æ–°åŠ è½½æ¨¡å‹"æŒ‰é’®"""


def clear_session():
    return "", None


def reinit_model(large_language_model, embedding_model, history):
    try:
        chat_llm.init_model_config(
            large_language_model=large_language_model, embedding_model=embedding_model
        )
        model_status = """æ¨¡å‹å·²æˆåŠŸé‡æ–°åŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
    except Exception as e:
        print(e)
        model_status = """æ¨¡å‹æœªæˆåŠŸé‡æ–°åŠ è½½ï¼Œè¯·ç‚¹å‡»é‡æ–°åŠ è½½æ¨¡å‹"""
    return history + [[None, model_status]]


def init_vector_store(file_obj):
    # åŠ è½½æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„å
    print(type(file_obj))
    file_input = None
    if file_obj is not None:
        file_input = file_obj.name

    vector_store = chat_llm.init_knowledge_vector_store(file_input)

    return vector_store


def predict(input, use_web, top_k, history_len, temperature, top_p, history=None):
    if history == None:
        history = []

    if use_web == "True":
        # ç½‘ç»œæœç´¢ä¿æŒå’Œåˆå§‹åŒ–çš„å¤§æ¨¡å‹æ˜¯åŒä¸€ä¸ª
        web_content = google_search(query=input, llm=chat_llm.llm)
    else:
        web_content = ""

    resp = chat_llm.get_knowledge_based_answer(
        query=input,
        web_content=web_content,
        top_k=top_k,
        history_len=history_len,
        temperature=temperature,
        top_p=top_p,
        history=history,
    )
    history.append((input, resp["result"]))
    return "", history, history


model_status = init_model()

if __name__ == "__main__":
    block = gr.Blocks()
    with block as demo:

        gr.Markdown(
            """<h1><center>LangChain-ChatLLM-Webui</center></h1>
        <center><font size=3>
        æœ¬é¡¹ç›®åŸºäºLangChainå’Œå¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—æ¨¡å‹, æä¾›åŸºäºæœ¬åœ°çŸ¥è¯†çš„è‡ªåŠ¨é—®ç­”åº”ç”¨. <br>
        ç›®å‰é¡¹ç›®æä¾›åŸºäº<a href='https://github.com/THUDM/ChatGLM-6B' target="_blank">ChatGLM-6B </a>çš„LLMå’ŒåŒ…æ‹¬GanymedeNil/text2vec-large-chineseã€nghuyong/ernie-3.0-base-zhã€nghuyong/ernie-3.0-nano-zhåœ¨å†…çš„å¤šä¸ªEmbeddingæ¨¡å‹, æ”¯æŒä¸Šä¼  txtã€docxã€mdã€pdfç­‰æ–‡æœ¬æ ¼å¼æ–‡ä»¶. <br>
        åç»­å°†æä¾›æ›´åŠ å¤šæ ·åŒ–çš„LLMã€Embeddingå’Œå‚æ•°é€‰é¡¹ä¾›ç”¨æˆ·å°è¯•, æ¬¢è¿å…³æ³¨<a href='https://github.com/thomas-yanxin/LangChain-ChatGLM-Webui' target="_blank">Githubåœ°å€</a>.
        </center></font>
        """
        )
        model_status = gr.State(model_status)
        with gr.Row():
            with gr.Column(scale=1):
                model_choose = gr.Accordion("æ¨¡å‹é€‰æ‹©")
                with model_choose:
                    large_language_model = gr.Dropdown(
                        choices=llm_model_list,
                        label="large language model",
                        value=init_llm,
                    )

                    embedding_model = gr.Dropdown(
                        list(embedding_model_dict.keys()),
                        label="Embedding model",
                        value=init_embedding_model,
                    )
                    load_model_button = gr.Button("é‡æ–°åŠ è½½æ¨¡å‹")
                model_argument = gr.Accordion("æ¨¡å‹å‚æ•°é…ç½®")
                with model_argument:

                    top_k = gr.Slider(
                        1,
                        10,
                        value=6,
                        step=1,
                        label="vector search top k",
                        interactive=True,
                    )

                    history_len = gr.Slider(
                        0, 5, value=3, step=1, label="history len", interactive=True
                    )

                    temperature = gr.Slider(
                        0,
                        1,
                        value=0.01,
                        step=0.01,
                        label="temperature",
                        interactive=True,
                    )
                    top_p = gr.Slider(
                        0, 1, value=0.9, step=0.1, label="top_p", interactive=True
                    )

                file = gr.File(
                    label="è¯·ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶",
                    file_types=[".txt", ".md", ".docx", ".pdf"],
                )

                init_vs = gr.Button("çŸ¥è¯†åº“æ–‡ä»¶å‘é‡åŒ–")
                # æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
                use_web = gr.Radio(["True", "False"], label="Web Search", value="False")

            with gr.Column(scale=4):
                chatbot = gr.Chatbot(
                    [[None, model_status.value]], label="ChatLLM", height=750
                )
                message = gr.Textbox(label="è¯·è¾“å…¥é—®é¢˜")
                state = gr.State()

                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    send = gr.Button("ğŸš€ å‘é€")

            load_model_button.click(
                reinit_model,
                show_progress=True,
                inputs=[large_language_model, embedding_model, chatbot],
                outputs=chatbot,
            )
            init_vs.click(
                init_vector_store,
                show_progress=True,
                inputs=[file],
                outputs=[],
            )

            send.click(
                predict,
                inputs=[
                    message,
                    use_web,
                    top_k,
                    history_len,
                    temperature,
                    top_p,
                    state,
                ],
                outputs=[message, chatbot, state],
            )
            clear_history.click(
                fn=clear_session, inputs=[], outputs=[chatbot, state], queue=False
            )

            message.submit(
                predict,
                inputs=[
                    message,
                    use_web,
                    top_k,
                    history_len,
                    temperature,
                    top_p,
                    state,
                ],
                outputs=[message, chatbot, state],
            )
        gr.Markdown(
            """æé†’ï¼š<br>
        1. ä½¿ç”¨æ—¶è¯·å…ˆä¸Šä¼ è‡ªå·±çš„çŸ¥è¯†æ–‡ä»¶ï¼Œå¹¶ä¸”æ–‡ä»¶ä¸­ä¸å«æŸäº›ç‰¹æ®Šå­—ç¬¦ï¼Œå¦åˆ™å°†è¿”å›error. <br>
        2. æœ‰ä»»ä½•ä½¿ç”¨é—®é¢˜ï¼Œè¯·é€šè¿‡[Github IssueåŒº](https://github.com/thomas-yanxin/LangChain-ChatGLM-Webui/issues)è¿›è¡Œåé¦ˆ. <br>
        """
        )
    # threads to consume the request
    demo.queue(max_size=20).launch(
        server_name="127.0.0.1",  # ip for listening, 0.0.0.0 for every inbound traffic, 127.0.0.1 for local inbound
        server_port=8042,  # the port for listening
        show_api=False,  # if display the api document
        share=False,  # if register a public url
        inbrowser=False,
        max_threads=3,
    )  # if browser would be open automatically
